= Kubernetes Cluster Logging using Fluentd, Elasticsearch and Kibana
:toc:
:icons:
:linkcss:
:imagesdir: ../images

== Kubernetes - Cluster Logging

There are many options for aggregating and streaming kubernetes logs. In this part of the workshop, we are going to focus on an AWS based deployment which consists of the following:

. https://www.fluentd.org/[Fluentd] is an open source data collector from 600+ types of systems for a unified logging layer.
. https://www.elastic.co/products/elasticsearch[Elasticsearch] is a distributed, RESTful search and analytics engine.
. https://www.elastic.co/products/kibana[Kibana] lets you visualize your Elasticsearch data.

Together, Fluentd, Elasticsearch and Kibana is also known as EFK stack.

In terms of architecture, Fluentd is deployed as a DaemonSet with the Cloud Watch logs plugin. Logs from different pods?? in the cluster will stream to a CloudWatch log group. An AWS Lambda function will stream the Log Group into an Amazon Elasticsearch cluster.

CloudWatch log group allow to combine log streams that share the same retention, monitoring, and access control settings.

=== CloudWatch Log Group

Cretea a CloudWatch log group:

    aws logs create-log-group --log-group-name kubernetes-logs --region us-east-1

=== Prepare Kubernetes manifest files

If you customized the log_group_name (from the previous command) or want to use a custom log_stream_name, you will need to update the `output.conf` section in `templates/fluentd-configmap.yaml`.

    output.conf: |
      <match **>
        # Plugin specific settings
        type cloudwatch_logs
        log_group_name kubernetes-cluster
        log_stream_name fluentd-cloudwatch-demo
        auto_create_stream true
        # Buffer settings
        buffer_chunk_limit 2M
        buffer_queue_limit 32
        flush_interval 10s
        max_retry_wait 30
        disable_retry_limit
        num_threads 8
      </match>

You will need to create an IAM user and set the AWS_ACCESS_KEY, AWS_SECRET_KEY and AWS_REGION in the ../templates/fluentd-ds.yaml file. Other options to accomplish this (in a production environment) are to use either kubernetes secrets or ec2 IAM roles.

  env:
  - name: FLUENTD_CONFIG
    value: fluentd-standalone.conf
  - name: AWS_REGION
    value: $REGION
  - name: AWS_ACCESS_KEY
    value: $ACCESS_KEY
  - name: AWS_SECRET_KEY
    value: $SECRET_KEY

=== Deploy Fluentd into Kubernetes

First create the logging namespace

    kubectl create ns logging

Create all of the necessary service accounts and roles:

    kubectl create -f ./templates/fluentd-service-account.yaml
    kubectl create -f ./templates/fluentd-role.yaml
    kubectl create -f ./templates/fluentd-role-binding.yaml

Then deploy Fluentd:

    kubectl create -f ./templates/fluentd-configmap.yaml
    kubectl create -f ./templates/fluentd-svc.yaml
    kubectl create -f ./templates/fluentd-ds.yaml

Watch for all of the pods to change to running status:

    kubectl get pods --watch --namespace=logging

We can now login to the AWS console -> Management Tools -> CloudWatch -> Logs -> kubernetes-cluster -> fluentd-cloudwatch-demo

We should start to see logs arrive into the service and can use the search feature to looks for specific logs

=== Provision an Amazon Elasticsearch cluster

This example creates a two instance Amazon Elasticsearch cluster named kops-logs in the Virgina region. Note that this cluster has an open access policy which will need to be locked down in production environments:

    aws es create-elasticsearch-domain --domain-name kubernetes-logs --elasticsearch-version 5.5 --elasticsearch-cluster-config  InstanceType=m4.large.elasticsearch,InstanceCount=2 --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 --access-policies '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"AWS":["*"]},"Action":["es:*"],"Resource":"*"}]}' --region us-east-1

Follow the instructions here to stream your logs from CloudWatch logs to your Amazon Elasticsearch cluster:

http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html

Now you can go to the Amazon Elasticsearch console and open the Kibana dashboard from the link and begin using the capabilites to search and visuzalize your Kubernetes cluster metrics

== Conclusion

In this post we demonstrated how to leverage AWS managed services to collect, search and visualize your kubernetes metrics. This can be used as reference to being to build your own logging solution for Kubernetes on top of AWS.
