= Kubernetes Cluster Logging using Fluentd, Elasticsearch and Kibana
:toc:
:icons:
:linkcss:
:imagesdir: ../images

== Kubernetes - Cluster Logging

There are many options for aggregating and streaming kubernetes logs. In this part of the workshop, we are going to focus on an AWS based deployment which consists of the following:

. https://www.fluentd.org/[Fluentd] is an open source data collector from 600+ types of systems for a unified logging layer.
. https://www.elastic.co/products/elasticsearch[Elasticsearch] is a distributed, RESTful search and analytics engine.
. https://www.elastic.co/products/kibana[Kibana] lets you visualize your Elasticsearch data.

Together, Fluentd, Elasticsearch and Kibana is also known as "`EFK stack`".

In terms of architecture, Fluentd is deployed as a DaemonSet with the Cloud Watch logs plugin. Logs from different pods?? in the cluster will stream to a CloudWatch log group. An AWS Lambda function will stream the Log Group into an Amazon Elasticsearch cluster. The logs can then be viewed using a Kibana dashboard.

=== Provision an Amazon Elasticsearch cluster

This example creates a two instance Amazon Elasticsearch cluster named `kubernetes-logs`. This cluster is created in the same region as the Kubernetes cluster and CloudWatch log group. Note that this cluster has an open access policy which will need to be locked down in production environments:

    aws es create-elasticsearch-domain \
      --domain-name kubernetes-logs \
      --elasticsearch-version 5.5 \
      --elasticsearch-cluster-config \
      InstanceType=m4.large.elasticsearch,InstanceCount=2 \
      --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \
      --access-policies '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"AWS":["*"]},"Action":["es:*"],"Resource":"*"}]}' \
      --region us-west-2

It takes a little while for the cluster to be created and come into `active` state. The AWS Consol should show the following status when the cluster is ready:

image::logging-cloudwatch-es-cluster.png[]

Look for the word `Active` in the right column.

=== CloudWatch Log Group

CloudWatch log group allow to combine log streams that share the same retention, monitoring, and access control settings.

Cretea a CloudWatch log group:

    aws logs create-log-group --log-group-name kubernetes-logs --region us-west-2

Create the log group in the same region as your cluster.

=== Prepare Kubernetes manifest files

==== Log group name and log stream name

Here is an excerpt from `templates/fluentd-configmap.yaml`:

    output.conf: |
      <match **>
        # Plugin specific settings
        type cloudwatch_logs
        log_group_name kubernetes-logs
        log_stream_name fluentd-cloudwatch
        auto_create_stream true
        # Buffer settings
        buffer_chunk_limit 2M
        buffer_queue_limit 32
        flush_interval 10s
        max_retry_wait 30
        disable_retry_limit
        num_threads 8
      </match>


It uses the log group name of `kubernetes-logs` and the log stream name of `fluentd-cloudwatch`. If a different log group name is used in the previous command or a different log stream name is needed, then that needs to be configured in this configuration file.

==== IAM configuration

You will need to create an IAM user and set the AWS_ACCESS_KEY, AWS_SECRET_KEY and AWS_REGION in the `templates/fluentd-ds.yaml` file. Other options to accomplish this (in a production environment) are to use either kubernetes secrets or EC2 IAM roles.

  env:
  - name: FLUENTD_CONFIG
    value: fluentd-standalone.conf
  - name: AWS_REGION
    value: $REGION
  - name: AWS_ACCESS_KEY
    value: $ACCESS_KEY
  - name: AWS_SECRET_KEY
    value: $SECRET_KEY

=== Deploy Fluentd into Kubernetes

First create the logging namespace

    kubectl create ns logging

Create all of the necessary service accounts and roles:

    kubectl create -f ./templates/fluentd-service-account.yaml
    kubectl create -f ./templates/fluentd-role.yaml
    kubectl create -f ./templates/fluentd-role-binding.yaml

Then deploy Fluentd:

    kubectl create -f ./templates/fluentd-configmap.yaml
    kubectl create -f ./templates/fluentd-svc.yaml
    kubectl create -f ./templates/fluentd-ds.yaml

Watch for all of the pods to change to running status:

    kubectl get pods --watch --namespace=logging

The output should look like:

    NAME            READY     STATUS    RESTARTS   AGE
    fluentd-7mxhg   1/1       Running   0          5s
    fluentd-nrz66   1/1       Running   0          5s

We can now login to the AWS console -> Management Tools -> CloudWatch -> Logs -> kubernetes-logs -> fluentd-cloudwatch

We should start to see logs arrive into the service and can use the search feature to looks for specific logs

=== Subscribe a CloudWatch Log Group to Amazon Elastisearch

CloudWatch Logs can be delivered to other services such as Amazon Elasticsearch for custom processing. This can be achieved by subscribing to a real-time feed of log events. A subscription filter defines the filter pattern to use for filtering which log events gets delivered to Elasticsearch, as well as information about where to send matching log events to.

In this section, we'll subscribe to the CloudWatch log events from the `fluent-cloudwatch` stream from the `kubernetes-logs` log group. This feed will be streamed to Elasticsearch cluster.

Original instructions for this are available at:

http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html

The instructions below show how this can be achieved for our setup:

. Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/.
. In the navigation pane, choose `Logs`.
. Select the log group to subscribe.
. Choose `Actions`, `Stream to Amazon Elasticsearch Service`.
+
image::logging-cloudwatch-es-subscribe.png[]
+
. Select the IAM role
+
image::logging-cloudwatch-es-subscribe-iam.png[]
+
. Click on `Next`.
. Select a Log Format:
+
image::logging-cloudwatch-es-subscribe-log-format.png[]
+
The fields that are sent to the Elasticsearch cluster can be selected. Optionally, you can select a log stream and then click on `Test Pattern` to verify that your search filter is returning the results you expect.
. Click on `Next`
. Review all the information:
+
image::logging-cloudwatch-es-subscribe-confirmation.png[]
+
. Click on `Start streaming`:
+
image::logging-cloudwatch-es-subscribe-start-streaming.png[]
+
. Cloudwatch page is refreshed to show that the filter was successfully created:
+
image::logging-cloudwatch-es-subscribe-filter-created.png[]

Now you can go to the Amazon Elasticsearch console and open the Kibana dashboard from the link and begin using the capabilites to search and visuzalize your Kubernetes cluster metrics

== Conclusion

In this post we demonstrated how to leverage AWS managed services to collect, search and visualize your kubernetes metrics. This can be used as reference to being to build your own logging solution for Kubernetes on top of AWS.
