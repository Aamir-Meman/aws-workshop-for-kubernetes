= Kubernetes Cluster Monitoring
:toc:
:icons:
:linkcss:
:imagesdir: ../images

== Introduction

This chapter will demonstrate how to monitor a Kubernetes cluster using the following:

. Kubernetes Dashboard
. Heapster, InfluxDB and Grafana
. Prometheus, Node exporter and Grafana

http://prometheus.io/[Prometheus] is an open-source systems monitoring and alerting toolkit. Prometheus collects metrics from monitored targets by scraping metrics from HTTP endpoints on these targets.

Heapster is limited to Kuberenetes container metrics, it is not general use. Heapster can be used as Prometheus scrape target.

== Pre-requisites

A 3 master nodes and 5 worker nodes cluster as explained at link:../cluster-install#multi-master-multi-node-multi-az-gossip-based-cluster[] is used for this chapter.

All configuration files for this chapter are in the `cluster-monitoring` directory.

== Kubernetes Dashboard

https://github.com/kubernetes/dashboard[Kubernetes Dashboard] is a general purpose web-based UI for Kubernetes clusters.

Deploy Dashboard using the following command:

    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml

Dashboard can be seen using the following command:

    kubectl proxy

Now, Dashboard is accessible at http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/.

Starting with Kubernetes 1.7, Dashboard supports authentication. Read more about it at https://github.com/kubernetes/dashboard/wiki/Access-control#introduction. We'll use a bearer token for authentication.

Check existing secrets in the `kube-system` namespace:

    kubectl -n kube-system get secret

It shows the output as:

```
NAME                                     TYPE                                  DATA      AGE
attachdetach-controller-token-dhkcr      kubernetes.io/service-account-token   3         3h
certificate-controller-token-p131b       kubernetes.io/service-account-token   3         3h
daemon-set-controller-token-r4mmp        kubernetes.io/service-account-token   3         3h
default-token-7vh0x                      kubernetes.io/service-account-token   3         3h
deployment-controller-token-jlzkj        kubernetes.io/service-account-token   3         3h
disruption-controller-token-qrx2v        kubernetes.io/service-account-token   3         3h
dns-controller-token-v49b6               kubernetes.io/service-account-token   3         3h
endpoint-controller-token-hgkbm          kubernetes.io/service-account-token   3         3h
generic-garbage-collector-token-34fvc    kubernetes.io/service-account-token   3         3h
horizontal-pod-autoscaler-token-lhbkf    kubernetes.io/service-account-token   3         3h
job-controller-token-c2s8j               kubernetes.io/service-account-token   3         3h
kube-dns-autoscaler-token-s3svx          kubernetes.io/service-account-token   3         3h
kube-dns-token-92xzb                     kubernetes.io/service-account-token   3         3h
kube-proxy-token-0ww14                   kubernetes.io/service-account-token   3         3h
kubernetes-dashboard-certs               Opaque                                2         9m
kubernetes-dashboard-key-holder          Opaque                                2         9m
kubernetes-dashboard-token-vt0fd         kubernetes.io/service-account-token   3         10m
namespace-controller-token-423gh         kubernetes.io/service-account-token   3         3h
node-controller-token-r6lsr              kubernetes.io/service-account-token   3         3h
persistent-volume-binder-token-xv30g     kubernetes.io/service-account-token   3         3h
pod-garbage-collector-token-fwmv4        kubernetes.io/service-account-token   3         3h
replicaset-controller-token-0cg8r        kubernetes.io/service-account-token   3         3h
replication-controller-token-3fwxd       kubernetes.io/service-account-token   3         3h
resourcequota-controller-token-6rl9f     kubernetes.io/service-account-token   3         3h
route-controller-token-9brzb             kubernetes.io/service-account-token   3         3h
service-account-controller-token-bqlsk   kubernetes.io/service-account-token   3         3h
service-controller-token-1qlg6           kubernetes.io/service-account-token   3         3h
statefulset-controller-token-kmgzg       kubernetes.io/service-account-token   3         3h
ttl-controller-token-vbnhf               kubernetes.io/service-account-token   3         3h
```

We can login using any secret with type 'kubernetes.io/service-account-token', though each of them have different privileges. In our case, we'll use the token from secret `default-token-7vh0x` to login. Use the following command to get the token for this secret:

    kubectl -n kube-system describe secret default-token-7vh0x

Note you'll need to replace `default-token-7vh0x` with the default-token from your output list.

It shows the output:

```
Name:         default-token-7vh0x
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name=default
              kubernetes.io/service-account.uid=3a3fea86-b3a1-11e7-9d90-06b1e747c654

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1046 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLTd2aDB4Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzYTNmZWE4Ni1iM2ExLTExZTctOWQ5MC0wNmIxZTc0N2M2NTQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.GHW-7rJcxmvujkClrN6heOi_RYlRivzwb4ScZZgGyaCR9tu2V0Z8PE5UR6E_3Vi9iBCjuO6L6MLP641bKoHB635T0BZymJpSeMPQ7t1F02BsnXAbyDFfal9NUSV7HoPAhlgURZWQrnWojNlVIFLqhAPO-5T493SYT56OwNPBhApWwSBBGdeF8EvAHGtDFBW1EMRWRt25dSffeyaBBes5PoJ4SPq4BprSCLXPdt-StPIB-FyMx1M-zarfqkKf7EJKetL478uWRGyGNNhSfRC-1p6qrRpbgCdf3geCLzDtbDT2SBmLv1KRjwMbW3EF4jlmkM4ZWyacKIUljEnG0oltjA
```

Copy the value of token from this output, select `Token` in the Dashboard login window, and paste the text. Click on `SIGN IN` to see the default Dashboard view:

image::kubernetes-dashboard-default.png[]

Click on `Nodes` to see a textual representation about the nodes running in the cluster:

image::monitoring-nodes-before.png[]

Install a Java application as explained in link:../helm[Deploying applications using Kubernetes Helm charts].

Click on `Pods`, again to see a textual representation about the pods running in the cluster:

image::monitoring-pods-before.png[]

This will change after Heapster, InfluxDB and Grafana are installed.

== Heapster, InfluxDB and Grafana

https://github.com/kubernetes/heapster[Heapster] is a metrics aggregator and processor. It is installed as a cluster-wide pod. It gathers monitoring and events data for all containers on each node by talking to the Kubelet. Kubelet itself fetches this data from https://github.com/google/cadvisor[cAdvisor]. This data is persisted in a time series database https://github.com/influxdata/influxdb[InfluxDB] for storage. The data is then visualized using a http://grafana.org/[Grafana] dashboard, or it can be viewed in Kubernetes Dashboard.

Heapster collects and interprets various signals like compute resource usage, lifecycle events, etc., and exports cluster metrics via REST endpoints.

Heapster, InfluxDB and Grafana are http://kubernetes.io/docs/admin/addons/[Kubernetes addons].

=== Installation

Execute this command to install Heapster, InfluxDB and Grafana:

  $ kubectl create -f templates/heapster/
  deployment "monitoring-grafana" created
  service "monitoring-grafana" created
  clusterrolebinding "heapster" created
  serviceaccount "heapster" created
  deployment "heapster" created
  service "heapster" created
  deployment "monitoring-influxdb" created
  service "monitoring-influxdb" created

Heapster is now aggregating metrics from the cAdvisor instances running on each node. This data is stored in an InfluxDB instance running in the cluster. Grafana dashboard, accessible at http://localhost:8001/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy/?orgId=1, now shows the information about the cluster. 

NOTE: Grafana dashboard will not be available if Kubernetes proxy is not running. If proxy is not running, it can be started with the command `kubectl proxy`.

=== Grafana dashboard

There are some built-in dashboards for monitoring the cluster and workloads. They are available by clicking on the upper left corner of the screen.

image::monitoring-grafana-dashboards.png[]

The "`Cluster`" dashboard shows all worker nodes, and their CPU and memory metrics. Type in a node name to see its collected metrics during a chosen period of time.

The cluster dashboard looks like this:

image::monitoring-grafana-dashboards-cluster.png[]

The "`Pods`"" dashboard allows you to see the resource utilization of every pod in the cluster. As with nodes, you can select the pod by typing its name in the top filter box.

image::monitoring-grafana-dashboards-pods.png[]

After the deployment of Heapster, Kubernetes Dashboard now shows additional graphs such as CPU and Memory utilization for pods and nodes, and other workloads.

The updated view of the cluster in Kubernetes Dashboard looks like this:

image::monitoring-nodes-after.png[]

The updated view of pods looks like this:

image::monitoring-pods-after.png[]

=== Cleanup

Remove all the installed components:

    kubectl delete -f templates/heapster/

== Prometheus, Node exporter and Grafana

http://prometheus.io/[Prometheus] is an open-source systems monitoring and alerting toolkit. Prometheus collects metrics from monitored targets by scraping metrics from HTTP endpoints on these targets.

Different targets to scrape are defined in a Prometheus configuration file. Targets may be statically configured via the `static_configs` parameter in the configuration file or dynamically discovered using one of the supported service-discovery mechanisms (Consul, DNS, Etcd, etc.).

https://github.com/prometheus/node_exporter[Node exporter] is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels.

=== Installation

The Prometheus configuration file is defined as a ConfigMap in the file `templates/prometheus/prometheus-configmap.yaml`.

We need to provide the location of the etcd server in our cluster in this configuration file. In our case, etcd is running inside the Kubernetes cluster. Find the IP address of etcd pods using this command:

  $ kubectl get pods --namespace=kube-system | grep etcd-server-ip
  etcd-server-ip-172-20-111-7.ec2.internal               1/1       Running   0          2h
  etcd-server-ip-172-20-48-45.ec2.internal               1/1       Running   0          2h
  etcd-server-ip-172-20-77-67.ec2.internal               1/1       Running   0          2h

There is one etcd server per master. So if you are using a three master cluster, as is the case here, then you'll see three entries starting with `etcd-server-ip`. Pick any pod.

Get more details about this pod:

  kubectl describe pod/etcd-server-ip-172-20-111-7.ec2.internal --namespace=kube-system 

The output looks like this:

```
Name:         etcd-server-ip-172-20-111-7.ec2.internal
Namespace:    kube-system
Node:         ip-172-20-111-7.ec2.internal/172.20.111.7
Start Time:   Fri, 03 Nov 2017 14:08:05 -0700
Labels:       k8s-app=etcd-server
Annotations:  kubernetes.io/config.hash=079fd5412e50b130fb994d547d5f8926
              kubernetes.io/config.mirror=079fd5412e50b130fb994d547d5f8926
              kubernetes.io/config.seen=2017-11-03T21:08:00.804876902Z
              kubernetes.io/config.source=file
Status:       Running
IP:           172.20.111.7
Containers:
  etcd-container:
    Container ID:  docker://9ffdd4ff0183ae44bbe09188c294a67fe2c470824f57ed7a5db1d9f7a8404527
    Image:         gcr.io/google_containers/etcd:2.2.1
    Image ID:      docker-pullable://gcr.io/google_containers/etcd@sha256:19544a655157fb089b62d4dac02bbd095f82ca245dd5e31dd1684d175b109947
    Ports:         2380/TCP, 4001/TCP

. . .

      ETCD_LISTEN_PEER_URLS:             http://0.0.0.0:2380
      ETCD_LISTEN_CLIENT_URLS:           http://0.0.0.0:4001
      ETCD_ADVERTISE_CLIENT_URLS:        http://etcd-c.internal.example.cluster.k8s.local:4001

. . .

QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     :NoExecute
Events:          <none>
```

From this output, note down the IP address and port on which etcd server is listening. This value is `172.20.111.7` and `4001` in our case.

Update the file `templates/prometheus/prometheus-configmap.yaml`, and replace `<IP>` with the IP address of the `etcd` server in your cluster. The updated fragment will look like as shown:

```
- job_name: 'etcd'
  target_groups:
  - targets:
    - 172.20.111.7:4001
```

Once you save the etcd information then you can deploy the ConfigMap:

  $ kubectl create -f templates/prometheus/prometheus-configmap.yaml
  configmap "prometheus" created

Next, deploy Prometheus into your cluster:

  $ kubectl create -f templates/prometheus/prometheus-deployment.yaml
  service "prometheus" created
  deployment "prometheus" created

Next, we will deploy the node exporter DaemonSet which will read system level metrics from each node and export them to Prometheus. Node exporter is defined as a DaemonSet, and so there is a single instance running on each node of the cluster:

  $ kubectl create -f templates/prometheus/node-exporter.yaml
  service "node-exporter" created
  daemonset "node-exporter" created

Finally, deploy the Grafana dashboard:

  $ kubectl create -f templates/prometheus/grafana.yml
  service "grafana" created
  deployment "grafana" created

=== Prometheus Dashboard

Prometheus is now scraping metrics from the etcd server, the Kubernetes API server and the node exporter. Metrics exported by different sources are listed below:

- etcd: https://coreos.com/etcd/docs/latest/metrics
- Kubernetes API server: https://github.com/kubernetes/kube-state-metrics
- Node exporter: https://github.com/prometheus/node_exporter

Let's look at these these metrics in the Prometheus dashboard. There are a few way to access the Prometheus dashboard?

You can use port forwarding. First find the pod name:

    $ kubectl get pods -l app=prometheus
    NAME                         READY     STATUS    RESTARTS   AGE
    prometheus-570506388-8z5hq   1/1       Running   0          1m

Then forward the traffic on that pod:

    $ kubectl port-forward prometheus-570506388-8z5hq 8080:9090 &

and enter http://127.0.0.1:8080/graph in your browser. Remember to replace the pod name in the `port-forward` command above.

Prometheus dashboard looks like:

image::prometheus-dashboard-initial.png[]

A wide set of metrics are available and can be seen in the dashboard. Here is a snapshot of metrics from etcd:

image::prometheus-dashboard-etcd.png[]

Here is a snapshot of metrics from the Kubernetes API server:

image::prometheus-dashboard-kubelet.png[]

Here is a snapshot of metrics from the node exporter:

image::prometheus-dashboard-node-exporter.png[]

=== Grafana dashboard

Start Kubernetes proxy, if not already running, using the command:

  kubectl proxy

Grafana dashboard is now accessible at http://localhost:8001/api/v1/proxy/namespaces/default/services/grafana/ and looks like as shown:

image::prometheus-grafana-dashboard-initial.png[]

Let's set up a dashboard which will show us some details about the cluster.

Click on top Grafana icon, and then select on Data Sources. Click on `+Add data source` to add a new data source. Specify the values as shown:

Click on `Add` to add the data source:

image::prometheus-grafana-add-data-source.png[]

Click on `Save & Test` to make sure that data source is working correctly:

image::prometheus-grafana-add-data-source-green.png[]

The green bar indicates that the data source is recognized.

Click on the Grafana icon again, select `Dashboards`, click on `Import`:

image::prometheus-grafana-import.png[]

We will use a pre-created https://grafana.com/dashboards/162[Kubernetes cluster monitoring] dashboard. A much larger set of dashboards are available at https://grafana.com/dashboards. Each such dashboard is assigned a unique number and can then be imported in any Grafana installation. The number in this case is `162`.

Copy that number in our Grafana UI, and click on `Load`:

image::prometheus-grafana-import-number.png[]

Select the `prometheus` source here, and click on `Save & Open`:

image::prometheus-grafana-import-save-open.png[]

The dashboard opens up in Grafana UI and looks like as shown:

image::prometheus-grafana-monitoring-dashboard.png[]

Multiple other dashboards are available at https://grafana.com/dashboards/, and can be imported in a similar fashion. Alternatively, you can create your own custom dashboars as explained in http://docs.grafana.org/guides/getting_started/[Getting Started].

=== Cleanup

Remove all the installed components:

    kubectl delete -f templates/prometheus/

