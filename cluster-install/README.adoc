= Create Kubernetes cluster using Kops
:toc:

This tutorial will walk you through how to install Kubernetes cluster using kops.

https://github.com/kubernetes/kops[Kops], short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud. A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons. After the cluster is created, the usual kubectl CLI can be used to manage resources in the cluster.

== Install kops

There is no need to download a Kubernetes binary distribution for creating a cluster using kops. However, you do need to download the kops CLI. It then takes care of downloading the right Kubernetes binary in the cloud, and provisions the cluster.

    brew update && brew install kops

In addition, you also need kubectl CLI to manage the resources on Kubernetes cluster. Instructions to install kubectl CLI are covered in link:../getting-started#setup-local-development-environment[].

== IAM user permission

Make sure the latest version of http://docs.aws.amazon.com/cli/latest/userguide/installing.html[AWS CLI]
is installed. User permissions being used must have these http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies.html[IAM policies] attached

    AmazonEC2FullAccess
    AmazonRoute53FullAccess
    AmazonS3FullAccess
    IAMFullAccess
    AmazonVPCFullAccess

Please review this link for additional info on IAM permission:
https://github.com/kubernetes/kops/blob/master/docs/aws.md#setup-iam-user

== S3 bucket to store Kubernetes config

Kops needs a "`state store`" to store configuration information of the cluster.  For example, how many nodes, instance type of each node, and Kubernetes version. The state is stored during the initial cluster creation. Any subsequent changes to the cluster are also persisted to this store as well. As of now, Amazon S3 is the only supported storage mechanism. Create a S3 bucket and pass that to the kops CLI during cluster creation.

    aws s3api create-bucket --bucket kubernetes-aws-io
    # enable versioning and export
    aws s3api put-bucket-versioning \
      --bucket kubernetes-aws-io \
      --versioning-configuration \
      Status=Enabled
    export KOPS_STATE_STORE=s3://kubernetes-aws-io

== Configure DNS

Creating a Kubernetes cluster using kops requires a top-level domain or a subdomain and setting up Route 53 hosted zones. This allows the various Kubernetes components to find and communicate with each other. This is also needed for kubectl to be able to talk directly with the master.

Kops also has an experimental support for gossip-based cluster. This does not require a top-level domain, subdomain or a Route53 hosted zone to be registered.

In order to create a cluster, you need to pick one of the options. These are described below.

=== Top-level domain or subdomain-based cluster

If you are using top-level domain or subdomain, and has any of the following scenarios:

. Domain purchased/hosted via AWS
. A subdomain under a domain purchased/hosted via AWS
. Setting up Route53 for a domain purchased with another registrar, transfering the domain to Route53
. Subdomain for clusters in Route53, leaving the domain at another registrar

Then you need to follow the instructions in https://github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns[configure DNS]. Typically, the first and the last bullet are common scenarios.

=== Gossip-based cluster

Kops also has an experimental support for gossip-based cluster. It uses Weave Mesh behind the scene. This makes the process of creating a Kubernetes cluster using kops DNS-free, and much more simplified. This also means a top-level domain or a subdomain is no longer required to create the cluster. The only requirement to trigger this is to have the cluster name end with `.k8s.local`. 

This is a fairly recent feature, so we recommend you continue to use DNS for production clusters. However, setting up a gossip-based cluster allows you to get started rather quickly.

=== Create private VPC

<to be fixed>
TODO: What is the need for it?

If you don't own a DNS domain name, you can create a custom domain using Route53's private hosted zone.
You need to provide VPC info to run this command.

     VPCID=`aws ec2 create-vpc --cidr-block 10.1.0.0/16 --region us-east-1 --query 'Vpc.VpcId' --output text`
     # modify dns hostname resolution for the VPC
     aws ec2 modify-vpc-attribute --vpc-id $VPCID --enable-dns-hostnames "{\"Value\":true}"
     # create internet gateway and attach it to VPC
     IGW=`aws ec2 create-internet-gateway --region us-east-1 --query 'InternetGateway.InternetGatewayId' --output text`
     aws ec2 attach-internet-gateway --internet $IGW --vpc $VPCID --region us-east-1

Now, create a Route53 private hosted zone using this VPC:

    ID=$(uuidgen) && aws route53 create-hosted-zone ]\
      --name k8s-aws.internal \
      --vpc VPCRegion=us-east-1,VPCId=$VPCID \
      --caller-reference $ID \
      | jq .DelegationSet.NameServers

</to be fixed>

== Create Kubernetes cluster

The Kops CLI can be used to create a highly available cluster, with multiple master nodes spread across multiple Availability Zones. Workers can be spread across multiple zones as well. Some of the tasks that happen behind the scene during cluster creation are:

- Provisioning EC2 instances
- Setting up AWS resources such as networks, Auto Scaling groups, IAM users, and security groups
- Installing Kubernetes

The examples below are using gossip-based protocol as opposed to using a top-level domain.

=== Default cluster

Create a simple Kubernetes cluster using the following command:

    kops create cluster \
      --name cluster.k8s.local \
      --zones us-east-1d,us-east-1e \
      --state s3://kubernetes-aws-io \
      --yes

The cluster name suffix of `.k8s.local` will result in the gossip protocol being used for master/node discovery. The `create cluster` command only creates and stores the cluster config in S3. Adding `--yes` option ensures that the cluster is immediately created as well.

Alternatively, you may not specify `--yes` option as part of the `kops create cluster` command. Then you can use `kops edit cluster cluster.k8s.local` command to view the current cluster state and make changes. The cluster creation, in that case, is started with the following command:

    kops update cluster cluster.k8s.local --yes

By default, this command creates a one master node and two worker nodes cluster.

Validate the cluster:

```
$ kops validate cluster
Using cluster from kubectl context: cluster.k8s.local

Validating cluster cluster.k8s.local

INSTANCE GROUPS
NAME      ROLE  MACHINETYPE MIN MAX SUBNETS
master-us-east-1d Master  m3.medium 1 1 us-east-1d
nodes     Node  t2.medium 2 2 us-east-1d,us-east-1e

NODE STATUS
NAME        ROLE  READY
ip-172-20-57-94.ec2.internal  master  True
ip-172-20-63-55.ec2.internal  node  True
ip-172-20-75-78.ec2.internal  node  True

Your cluster cluster.k8s.local is ready
```

More options for creating Kubernetes clusters are listed below.

=== Multi-master, multi-worker cluster, highly available

Check the list of Availability Zones available using the following command:

    aws --region <zone> ec2 describe-availability-zones

Create a cluster with multi master, multi node and multi-az configuration. As with the example above, this will create a cluster using the gossip protocol:

    kops create cluster \
      --name cluster.k8s.local \
      --master-count 3 \
      --node-count 5 \
      --zones us-east-1a,us-east-1b,us-east-1c \
      --state s3://kubernetes-aws-io \
      --yes

A multi-master cluster can be created by using the `--master-count` option and using an odd number value. The list of zones is defaulted from the value of `--zones` option. Alternatively, the AZs for master can be specified using the `--master-zones` option/ Kops will spread out the servers across different AZs.

`--zones` is used to distribute the worker nodes. The number of workers can be specified using the `--node-count` option.

Validate the cluster:

```
$ kops validate cluster
Using cluster from kubectl context: cluster.k8s.local

Validating cluster cluster.k8s.local

INSTANCE GROUPS
NAME      ROLE  MACHINETYPE MIN MAX SUBNETS
master-us-east-1a Master  m3.medium 1 1 us-east-1a
master-us-east-1b Master  m3.medium 1 1 us-east-1b
master-us-east-1c Master  c4.large  1 1 us-east-1c
nodes     Node  t2.medium 5 5 us-east-1a,us-east-1b,us-east-1c

NODE STATUS
NAME        ROLE  READY
ip-172-20-101-97.ec2.internal node  True
ip-172-20-119-53.ec2.internal node  True
ip-172-20-124-138.ec2.internal  master  True
ip-172-20-35-15.ec2.internal  master  True
ip-172-20-63-104.ec2.internal node  True
ip-172-20-69-241.ec2.internal node  True
ip-172-20-84-65.ec2.internal  node  True
ip-172-20-93-167.ec2.internal master  True

Your cluster cluster.k8s.local is ready
```

Note that all masters are spread across different AZs.

=== Multi-master, multi-worker cluster, highly available, with private VPC

<to be fixed>

Create a cluster with Route53 private hosted zone and VPC. For this to work you must have created the Route53 private hosted zone, as explained in <<Create hosted zone on Route53>>:

    kops create cluster \
      --dns private \
      --name cluster03.k8s-aws.internal \
      --zones us-east-1a,us-east-1b \
      --state s3://kubernetes-aws-config \
      --vpc $VPCID \
      --network-cidr 10.1.0.0/16 \
      --ssh-public-key $mypubkey

TIP: You may need to add cluster API endpoints into your hosts file (/etc/hosts) if you use Route53
private hosted zone along with VPC option.

</to be fixed>

== Delete cluster

Any cluster can be deleted as shown:

    kops delete cluster \
      <cluster-name> \
      --state s3://kubernetes-aws-io \
      --yes

`<cluster-name>` is the name of the cluster. For example, our `cluster.k8s.local` cluster can be deleted as:

    kops delete cluster \
      cluster.k8s.local \
      --state s3://kubernetes-aws-io \
      --yes

<to be fixed>

If you created a private VPC, then an additional cleanup of resources is required as shown below:

    # Find Route53 hosted zone ID from the console or via CLI and delete hosted zone
    aws route53 delete-hosted-zone --id Z1234567890ABC
    # Delete VPC if you created earlier
    aws ec2 detach-internet-gateway --internet $IGW --vpc $VPCID --region us-east-1
    aws ec2 delete-internet-gateway --internet-gateway-id $IGW
    aws ec2 delete-vpc --vpc-id $VPCID

</to be fixed>